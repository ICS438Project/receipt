{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a6085c",
   "metadata": {},
   "source": [
    "# 0) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5dcebf",
   "metadata": {},
   "source": [
    "## Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f37afca8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.4-cp311-cp311-win_amd64.whl (10.8 MB)\n",
      "     ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/10.8 MB 393.8 kB/s eta 0:00:28\n",
      "     --------------------------------------- 0.1/10.8 MB 819.2 kB/s eta 0:00:14\n",
      "      -------------------------------------- 0.2/10.8 MB 952.6 kB/s eta 0:00:12\n",
      "      -------------------------------------- 0.2/10.8 MB 962.7 kB/s eta 0:00:11\n",
      "     - -------------------------------------- 0.3/10.8 MB 1.0 MB/s eta 0:00:11\n",
      "     - -------------------------------------- 0.4/10.8 MB 1.1 MB/s eta 0:00:10\n",
      "     - -------------------------------------- 0.4/10.8 MB 1.1 MB/s eta 0:00:10\n",
      "     - -------------------------------------- 0.5/10.8 MB 1.2 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 0.5/10.8 MB 1.1 MB/s eta 0:00:10\n",
      "     -- ------------------------------------- 0.6/10.8 MB 1.2 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 0.7/10.8 MB 1.2 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 0.7/10.8 MB 1.2 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 0.8/10.8 MB 1.2 MB/s eta 0:00:09\n",
      "     --- ------------------------------------ 0.9/10.8 MB 1.2 MB/s eta 0:00:09\n",
      "     --- ------------------------------------ 0.9/10.8 MB 1.2 MB/s eta 0:00:09\n",
      "     --- ------------------------------------ 1.0/10.8 MB 1.2 MB/s eta 0:00:09\n",
      "     --- ------------------------------------ 1.0/10.8 MB 1.2 MB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 1.1/10.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ---- ----------------------------------- 1.2/10.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ---- ----------------------------------- 1.2/10.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ---- ----------------------------------- 1.3/10.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ---- ----------------------------------- 1.3/10.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ----- ---------------------------------- 1.4/10.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ----- ---------------------------------- 1.5/10.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ----- ---------------------------------- 1.5/10.8 MB 1.3 MB/s eta 0:00:08\n",
      "     ----- ---------------------------------- 1.6/10.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ------ --------------------------------- 1.6/10.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ------ --------------------------------- 1.7/10.8 MB 1.3 MB/s eta 0:00:08\n",
      "     ------ --------------------------------- 1.8/10.8 MB 1.3 MB/s eta 0:00:08\n",
      "     ------ --------------------------------- 1.8/10.8 MB 1.3 MB/s eta 0:00:08\n",
      "     ------ --------------------------------- 1.9/10.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ------- -------------------------------- 1.9/10.8 MB 1.3 MB/s eta 0:00:08\n",
      "     ------- -------------------------------- 2.0/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 2.1/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 2.1/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     -------- ------------------------------- 2.2/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     -------- ------------------------------- 2.3/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     -------- ------------------------------- 2.3/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     -------- ------------------------------- 2.4/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     --------- ------------------------------ 2.4/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     --------- ------------------------------ 2.5/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     --------- ------------------------------ 2.6/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     --------- ------------------------------ 2.6/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     --------- ------------------------------ 2.7/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     ---------- ----------------------------- 2.7/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     ---------- ----------------------------- 2.8/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     ---------- ----------------------------- 2.8/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     ---------- ----------------------------- 2.9/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     ----------- ---------------------------- 3.0/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     ----------- ---------------------------- 3.0/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     ----------- ---------------------------- 3.1/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 3.2/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     ----------- ---------------------------- 3.2/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 3.3/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 3.3/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 3.4/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 3.5/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 3.5/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 3.6/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 3.7/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 3.7/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 3.8/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 3.8/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 3.9/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 4.0/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 4.0/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "     --------------- ------------------------ 4.1/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "     --------------- ------------------------ 4.2/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "     --------------- ------------------------ 4.2/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "     --------------- ------------------------ 4.3/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "     ---------------- ----------------------- 4.3/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "     ---------------- ----------------------- 4.4/10.8 MB 1.3 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 4.5/10.8 MB 1.3 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 4.5/10.8 MB 1.3 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 4.6/10.8 MB 1.3 MB/s eta 0:00:05\n",
      "     ----------------- ---------------------- 4.6/10.8 MB 1.3 MB/s eta 0:00:05\n",
      "     ----------------- ---------------------- 4.7/10.8 MB 1.3 MB/s eta 0:00:05\n",
      "     ----------------- ---------------------- 4.8/10.8 MB 1.3 MB/s eta 0:00:05\n",
      "     ----------------- ---------------------- 4.8/10.8 MB 1.3 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 4.9/10.8 MB 1.3 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 5.0/10.8 MB 1.3 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 5.0/10.8 MB 1.3 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 5.1/10.8 MB 1.3 MB/s eta 0:00:05\n",
      "     ------------------- -------------------- 5.2/10.8 MB 1.3 MB/s eta 0:00:05\n",
      "     ------------------- -------------------- 5.2/10.8 MB 1.3 MB/s eta 0:00:05\n",
      "     ------------------- -------------------- 5.3/10.8 MB 1.3 MB/s eta 0:00:05\n",
      "     ------------------- -------------------- 5.3/10.8 MB 1.3 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 5.4/10.8 MB 1.3 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 5.4/10.8 MB 1.3 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 5.5/10.8 MB 1.3 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 5.6/10.8 MB 1.3 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 5.7/10.8 MB 1.3 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 5.7/10.8 MB 1.3 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 5.8/10.8 MB 1.3 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 5.8/10.8 MB 1.3 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 5.9/10.8 MB 1.3 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 5.9/10.8 MB 1.3 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 6.0/10.8 MB 1.3 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 6.1/10.8 MB 1.3 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 6.1/10.8 MB 1.3 MB/s eta 0:00:04\n",
      "     ----------------------- ---------------- 6.2/10.8 MB 1.3 MB/s eta 0:00:04\n",
      "     ----------------------- ---------------- 6.3/10.8 MB 1.3 MB/s eta 0:00:04\n",
      "     ----------------------- ---------------- 6.3/10.8 MB 1.3 MB/s eta 0:00:04\n",
      "     ----------------------- ---------------- 6.4/10.8 MB 1.3 MB/s eta 0:00:04\n",
      "     ----------------------- ---------------- 6.5/10.8 MB 1.3 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 6.5/10.8 MB 1.3 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 6.6/10.8 MB 1.3 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 6.6/10.8 MB 1.3 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 6.7/10.8 MB 1.3 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 6.7/10.8 MB 1.3 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 6.8/10.8 MB 1.3 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 6.9/10.8 MB 1.3 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 6.9/10.8 MB 1.3 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 7.0/10.8 MB 1.3 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 7.1/10.8 MB 1.3 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 7.1/10.8 MB 1.3 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 7.2/10.8 MB 1.3 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 7.2/10.8 MB 1.3 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 7.3/10.8 MB 1.3 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 7.4/10.8 MB 1.3 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 7.4/10.8 MB 1.3 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 7.5/10.8 MB 1.3 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 7.5/10.8 MB 1.3 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 7.6/10.8 MB 1.3 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 7.6/10.8 MB 1.3 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 7.7/10.8 MB 1.3 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 7.8/10.8 MB 1.3 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 7.8/10.8 MB 1.3 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 7.9/10.8 MB 1.3 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 8.0/10.8 MB 1.3 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 8.0/10.8 MB 1.3 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 8.1/10.8 MB 1.3 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 8.1/10.8 MB 1.3 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 8.2/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 8.3/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 8.3/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 8.4/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 8.5/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 8.5/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 8.6/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.6/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.7/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 8.9/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 8.9/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 9.0/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 9.1/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 9.1/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 9.2/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 9.2/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 9.3/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 9.4/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 9.4/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 9.5/10.8 MB 1.3 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 9.5/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 9.6/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 9.6/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 9.7/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 9.8/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 9.8/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 9.9/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 10.0/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 10.0/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 10.1/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 10.1/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 10.2/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 10.3/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 10.3/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 10.4/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 10.5/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.5/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.6/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.6/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.7/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.8/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 10.8/10.8 MB 1.3 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n"
     ]
    }
   ],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4959b784",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import time\n",
    "import faiss\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from jsonschema import validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682f0606",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae2821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These processes take a lot of time to set up \n",
    "UPDATE_RECEIPTS = False # Set this to true if we've added new reciept data in\n",
    "UPDATE_VENDOR_DATABASE = True # Set this to true if we've added more categories/examples to the vendor database\n",
    "UPDATE_PRODUCT_DATABASE = False # Set this to true if we've added more categories/examples to the product database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b99e27e",
   "metadata": {},
   "source": [
    "# 1) Use ChatGPT to Convert Receipt Text into Structured JSON\n",
    "\n",
    "* Make sure it generates correct data (use asserts to test all of this)\n",
    "* Make sure edge cases are handled (ex: blank fields, fields not in correct datatype, dollar sign in total, phone number larger than 10 digits)\n",
    "* Prevent language model from returning invalid json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991c820a",
   "metadata": {},
   "source": [
    "## Receipts Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c922f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECEIPTS_INPUT = './receipts/text'\n",
    "RECEIPTS_OUTPUT = './processed_receipts'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bec517",
   "metadata": {},
   "source": [
    "## OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11da93e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38501d6c",
   "metadata": {},
   "source": [
    "## ChatGPT Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81ee6d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHATGPT_PROMPT = '''Please analyze the provided receipt and extract relevant information to fill in the following structured format:\n",
    "{\n",
    "  \"ReceiptInfo\": {\n",
    "    \"merchant\": \"(string value)\",\n",
    "    \"address\": \"(string value)\", (split into street address, city, and state)\n",
    "    \"city\": \"(string value)\",\n",
    "    \"state\": \"(string value)\",\n",
    "    \"phoneNumber\": \"(string value)\",\n",
    "    \"tax\": \"(float value)\", (in dollars)\n",
    "    \"total\": \"(float value)\", (in dollars)\n",
    "    \"receiptDate\": \"(string value)\",\n",
    "    \"receiptTime\": \"(string value)\", (if available)\n",
    "    \"ITEMS\": [\n",
    "      {\n",
    "        \"description\": \"(string value)\",\n",
    "        \"quantity\": \"(integer value)\",\n",
    "        \"unitPrice\": \"(float value)\",\n",
    "        \"totalPrice\": \"(float value)\",\n",
    "        \"discountAmount\": \"(float value)\" if any\n",
    "      }, ...\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "Remember to check for any discounts or special offers applied to the items and reflect these in the item details. Make sure to end the json object and make sure it's in json format.\n",
    "1. tax, total, unitPrice, totalPrice, discountAmount in float value, and quantity in integer value\n",
    "2. ignore all <UNKNOWN> in the text\n",
    "3. Your response should start with { and end with },\n",
    "4. make sure close all ReceiptInfo and use , to separate different ReceiptInfo\n",
    "\n",
    "example: \"\"\"Marley's Shop\n",
    "123 Long Rd\n",
    "Kailua, HI 67530\n",
    "(808) 555-1234\n",
    "CASHIER: JOHN\n",
    "REGISTER #: 6\n",
    "04/12/2023\n",
    "Transaction ID: 5769009\n",
    "PRICE   QTY  TOTAL\n",
    "APPLES (1 lb)\n",
    "2.99 2 5.98  1001\n",
    "-1.00  999\n",
    "Choco Dream Cookies\n",
    "7.59 1 7.59   1001\n",
    "SUBTOTAL\n",
    "13.57\n",
    "SALES TAX 8.5%\n",
    "1.15\n",
    "TOTAL\n",
    "-14.72\n",
    "VISA CARD            14.72\n",
    "CARD#: **1234\n",
    "REFERENCE#: 6789\n",
    "THANK YOU FOR SHOPPING WITH US!\n",
    "\"\"\"\n",
    "\n",
    "from example should get:\n",
    "{\n",
    "  \"ReceiptInfo\": {\n",
    "    \"merchant\": \"Marley's Shop\",\n",
    "    \"address\": \"123 Long Rd\",\n",
    "    \"city\": \"Kailua\",\n",
    "    \"state\": \"HI\",\n",
    "    \"phoneNumber\": \"(xxx) xxx-xxxx\",\n",
    "    \"tax\": 1.15,\n",
    "    \"total\": 14.72,\n",
    "    \"receiptDate\": \"04/12/2023\",\n",
    "    \"receiptTime\": \"Transaction ID: 5769009\",\n",
    "    \"ITEMS\": [\n",
    "      {\n",
    "        \"description\": \"APPLES (1 lb)\",\n",
    "        \"quantity\": 2,\n",
    "        \"unitPrice\": 2.99,\n",
    "        \"totalPrice\": 5.98,\n",
    "        \"discountAmount\": 1.00\n",
    "      },\n",
    "      {\n",
    "        \"description\": \"Choco Dream Cookies\",\n",
    "        \"quantity\": 1,\n",
    "        \"unitPrice\": 7.59,\n",
    "        \"totalPrice\": 7.59,\n",
    "        \"discountAmount\": 0\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0025a1cd",
   "metadata": {},
   "source": [
    "## Functions to Convert Receipt Text into JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a49590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_files(folder_path):\n",
    "    '''\n",
    "    Reads all text files within a folder path.\n",
    " \n",
    "    Parameters:\n",
    "    folder_path (str): The folder path.\n",
    " \n",
    "    Returns:\n",
    "    list[str]: The list of all file names contained at the folder path.\n",
    "    '''\n",
    "    \n",
    "    text_list = []\n",
    "\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print('Invalid folder path.')\n",
    "        return None\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        if os.path.isfile(file_path) and filename.endswith('.txt'):\n",
    "            with open(file_path, 'r') as file:\n",
    "                file_content = file.read()\n",
    "                text_list.append(file_content) # Append file content as a string to the list\n",
    "                \n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd36ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_validate_json(response, schema):\n",
    "    '''\n",
    "    Processes and validates a JSON string.\n",
    " \n",
    "    Parameters:\n",
    "    response (str): The folder path.\n",
    "    schema (dict): The schema to validate against.\n",
    " \n",
    "    Returns:\n",
    "    dict or None: The JSON as a dictionary or None if invalid JSON.\n",
    "    '''\n",
    "    \n",
    "    # Find the index of the first '{'\n",
    "    brace_index = response.find('{')\n",
    "    \n",
    "    # If '{' is found and it's not the first character\n",
    "    if brace_index != -1:\n",
    "        # Extract JSON from the substring starting from the first '{'\n",
    "        extracted_json = response[brace_index:]\n",
    "        \n",
    "        # Validate the extracted JSON against the provided schema\n",
    "        try:\n",
    "            validate(instance=json.loads(extracted_json), schema=schema)\n",
    "            return extracted_json\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f'Error decoding JSON: {e}')\n",
    "        except ValidationError as e:\n",
    "            print(f'JSON validation error: {e}')\n",
    "    \n",
    "    # Return None if '{' is not found or it's the first character\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9529686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_receipt_json(receipt_text):\n",
    "    '''\n",
    "    Generates a receipt JSON given receipt text using ChatGPT.\n",
    " \n",
    "    Parameters:\n",
    "    receipt_text (str): The text to feed ChatGPT.\n",
    "\n",
    "    Returns:\n",
    "    dict or None: The receipt JSON as a dictionary or None if ChatGPT generates invalid JSON.\n",
    "    '''\n",
    "    \n",
    "    llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0, openai_api_key=OPENAI_API_KEY, max_tokens=1056)\n",
    "    response = llm(receipt_text)\n",
    "    \n",
    "    schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"ReceiptInfo\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"merchant\": {\"type\": \"string\"},\n",
    "                    \"address\": {\"type\": \"string\"},\n",
    "                    \"city\": {\"type\": \"string\"},\n",
    "                    \"state\": {\"type\": \"string\"},\n",
    "                    \"phoneNumber\": {\"type\": \"string\"},\n",
    "                    \"tax\": {\"type\": \"number\"},\n",
    "                    \"total\": {\"type\": \"number\"},\n",
    "                    \"receiptDate\": {\"type\": \"string\"},\n",
    "                    \"ITEMS\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"description\": {\"type\": \"string\"},\n",
    "                                \"quantity\": {\"type\": \"number\"},\n",
    "                                \"unitPrice\": {\"type\": \"number\"},\n",
    "                                \"totalPrice\": {\"type\": \"number\"},\n",
    "                                \"discountAmount\": {\"type\": \"number\"}\n",
    "                            },\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    return process_and_validate_json(response, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b45b5045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def receipt_texts_to_json_list():\n",
    "    '''\n",
    "    Converts all receipt texts located at RECEIPTS_INPUT into a file of a list of JSONs named entities.json.\n",
    "    '''\n",
    "              \n",
    "    output_path = RECEIPTS_OUTPUT + '/receipts.json'\n",
    "\n",
    "    receipts = read_text_files(RECEIPTS_INPUT)\n",
    "\n",
    "    receipts_json = []\n",
    "    errorReceipts = []\n",
    "    files_processed = 0\n",
    "    for receipt in receipts:\n",
    "        receipt_json = json.loads(generate_receipt_json(CHATGPT_PROMPT + receipt))\n",
    "        receipts_json.append(receipt_json)\n",
    "        files_processed += 1\n",
    "        \n",
    "        #Uncomment if using the free OpenAI API key\n",
    "        '''\n",
    "        if files_processed % 3 == 0:\n",
    "            time.sleep(60)\n",
    "        '''\n",
    "\n",
    "    with open(output_path, 'w') as file:\n",
    "        json.dump(receipts_json, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6af1c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def receipts_json_to_csv():\n",
    "    '''\n",
    "    Converts JSON list of receipts stored in entities.json into CSV of only vendor and product descriptions.\n",
    "    '''\n",
    "              \n",
    "    # Read and parse the JSON file\n",
    "    with open(RECEIPTS_OUTPUT + '/receipts.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "              \n",
    "    entry_number = 0\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    merchants = []\n",
    "    descriptions = []\n",
    "\n",
    "    # Iterate through the data\n",
    "    for entry in data:\n",
    "        entry_number += 1 \n",
    "        merchant = entry[\"ReceiptInfo\"][\"merchant\"]\n",
    "        items = entry[\"ReceiptInfo\"][\"ITEMS\"]\n",
    "\n",
    "        # Initialize a list to store cleaned descriptions for this entry\n",
    "        cleaned_descriptions = []\n",
    "\n",
    "        # Remove \"number+space\" occurrences in the descriptions and add to the list\n",
    "        for item in items:\n",
    "            description = item.get('description', 'No Description')\n",
    "            cleaned_description = ' '.join(word for word in description.split() if not word.isdigit())\n",
    "            cleaned_descriptions.append(cleaned_description)\n",
    "\n",
    "        # Remove \"UNKNOWN,\" \"<UNKNOWN>,\" and \"unknown\" from the merchant field\n",
    "        merchant = merchant.replace(\"UNKNOWN\", \"\").replace(\"<UNKNOWN>\", \"\").replace(\"unknown\", \"\").replace(\"<>\", \"\")\n",
    "\n",
    "        # Add the merchant and descriptions to the respective lists\n",
    "        merchants.append(merchant)\n",
    "        descriptions.append(cleaned_descriptions)\n",
    "\n",
    "    # Create a DataFrame and save as CSV\n",
    "    entities_df = pd.DataFrame({\n",
    "        'Vendors': merchants, \n",
    "        'Products': descriptions\n",
    "    })\n",
    "    entities_df.to_csv(RECEIPTS_OUTPUT + '/vendors_and_products.csv', index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a822b3",
   "metadata": {},
   "source": [
    "## Convert all Receipts into a List of JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf8a4620",
   "metadata": {},
   "outputs": [],
   "source": [
    "if UPDATE_RECEIPTS:\n",
    "    receipt_texts_to_json_list()\n",
    "    receipts_json_to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e926b32",
   "metadata": {},
   "source": [
    "# 2) Create vector databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efc5ed1",
   "metadata": {},
   "source": [
    "## Load BertTokenizer and BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9bf8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BAAI/bge-large-en\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd2a3ab",
   "metadata": {},
   "source": [
    "## Functions to Convert Word into Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5c49036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(word):\n",
    "    '''\n",
    "    Generates a vector of embeddings given a word/sentence.\n",
    " \n",
    "    Parameters:\n",
    "    word (str): The word/sentence.\n",
    "\n",
    "    Returns:\n",
    "    tensor(1, 1024): The vector of embeddings.\n",
    "    '''\n",
    "    \n",
    "    inputs = tokenizer(word, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling of token embeddings\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01865381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_embeddings_df(df):\n",
    "    '''\n",
    "    Convert words in a DataFrame column to embeddings.\n",
    " \n",
    "    Parameters:\n",
    "    df (dataframe): The dataframe.\n",
    "\n",
    "    Returns:\n",
    "    dataframe: The dataframe of embeddings.\n",
    "    '''\n",
    "    \n",
    "    embeddings = [generate_embeddings(x) for x in df.iloc[:, 0]] \n",
    "    dfs = []\n",
    "    for embedding in embeddings:\n",
    "        dfs.append(pd.DataFrame(embedding))\n",
    "    return pd.concat(dfs)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "399dfb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedded_vendor_database():\n",
    "    '''\n",
    "    Create vector of embeddings database from vendor word databases. \n",
    "    Outputs to ./databases/vendor/embedding.\n",
    "    '''\n",
    "    \n",
    "    vendor_database = pd.DataFrame()\n",
    "    \n",
    "    csv_files = glob.glob(os.path.join('.', 'databases', 'vendor', 'word', '*.csv'))\n",
    "    for file in csv_files:\n",
    "        category = os.path.split(file)[-1]\n",
    "        category_name = category.replace('.csv', '').replace('_', ' ')\n",
    "        \n",
    "        new_category = pd.read_csv(file, encoding='latin-1')\n",
    "        new_column = convert_to_embeddings_df(new_category)\n",
    "        new_column['Category'] = category_name\n",
    "        \n",
    "        vendor_database = pd.concat([vendor_database, new_column], ignore_index=True, axis=0)\n",
    "    vendor_database.to_csv(\"./databases/vendor/embedding/embedded_vendor_database.csv\")\n",
    "\n",
    "    return vendor_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e898b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedded_product_database():\n",
    "    '''\n",
    "    Create vector of embeddings database from product word databases. \n",
    "    Outputs to ./databases/product/embedding.\n",
    "    '''\n",
    "    \n",
    "    product_database = pd.DataFrame()\n",
    "    \n",
    "    # Loop through subfolders of product CSV files\n",
    "    for root, dirs, files in os.walk(os.path.join('.', 'databases', 'product', 'word')): \n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                csv_file_path = os.path.join(root, file) # Get the absolute path of the CSV file\n",
    "                category = os.path.split(file)[-1]\n",
    "                category_name = category.replace('.csv', '').replace('_', ' ')\n",
    "                \n",
    "                new_category = pd.read_csv(csv_file_path, encoding='latin-1')\n",
    "                new_column = convert_to_embeddings_df(new_category)\n",
    "                new_column['Category'] = category_name\n",
    "                \n",
    "                product_database = pd.concat([product_database, new_column], ignore_index=True, axis=0)\n",
    "    product_database.to_csv('./databases/product/embedding/embedded_product_database.csv')\n",
    "        \n",
    "    return product_database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d551cbd1",
   "metadata": {},
   "source": [
    "## Create vector database for vendors and output to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e36e90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if UPDATE_VENDOR_DATABASE:\n",
    "    create_embedded_vendor_database()\n",
    "\n",
    "if UPDATE_PRODUCT_DATABASE:\n",
    "    create_embedded_product_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619d745e",
   "metadata": {},
   "source": [
    "# 3) Vendor and Product Category Receipt Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c90e4f4",
   "metadata": {},
   "source": [
    "## Split Vector Database into X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcc3803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_database(file_path):\n",
    "    '''\n",
    "    Splits vector database csv into X and y.\n",
    " \n",
    "    Parameters:\n",
    "    file_path (str): The path to the vector database.\n",
    "\n",
    "    Returns:\n",
    "    tuple(dataframe, dataframe): X and y.\n",
    "    '''\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "    \n",
    "    # Creating variables from database values\n",
    "    X = df.drop('Category', axis=1)\n",
    "    y = df['Category']\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c44d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_list(row):\n",
    "    '''\n",
    "    Helper function to add vendor to product description to improve classifcation performance.\n",
    " \n",
    "    Parameters:\n",
    "    row (dataframe): The row of a dataframe.\n",
    "    \n",
    "    Returns:\n",
    "    tuple(list[str], list[str]): The X_test of vendor and product description combined \n",
    "                                 and the product decscription themselves.\n",
    "    '''\n",
    "    \n",
    "    X_test, items = [], []\n",
    "    for item in row['Products']:\n",
    "        X_test.append(item + \" \" + row['Vendors'])\n",
    "        items.append(item)\n",
    "    return X_test, items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376d1992",
   "metadata": {},
   "source": [
    "## Perform Cosine Similarity Classifcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d88db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_clf(X_train, y_train, X_test):\n",
    "    # Normalize the training datasets\n",
    "    X_train_norm = X_train / np.linalg.norm(X_train, axis=1)[:, None]\n",
    "    X_test_norm = X_test / np.linalg.norm(X_test, axis=1)[:, None]\n",
    "\n",
    "    dim = X_train_norm.shape[1]  \n",
    "    # Using IndexFlatIP (cosine similarity from faiss)\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(X_train_norm.astype(np.float32))\n",
    "\n",
    "    # Search for k nearest neighbors\n",
    "    k = 1 \n",
    "    _, indices = index.search(X_test_norm.astype(np.float32), k)\n",
    "    predicted_labels = y_train.iloc[indices.flatten()].values\n",
    "\n",
    "    return predicted_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f00613",
   "metadata": {},
   "source": [
    "## Run Vendor Category Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1305d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vendor_category(): \n",
    "    '''\n",
    "    Runs classification of vendor category on all receipts.\n",
    "    Outputs prediction results to ./predictions/vendor_category_predictions.csv.\n",
    "    '''\n",
    "    X_train, y_train = split_database('./databases/vendor/embedding/embedded_vendor_database.csv')\n",
    "    \n",
    "    receipts = pd.read_csv(\"./processed_receipts/vendors_and_products.csv\")\n",
    "    vendors = receipts['Vendors'].to_frame()\n",
    "    receipts['Products'] = receipts['Products'].apply(eval)\n",
    "    vendors = receipts.apply(process_list, axis=1)\n",
    "    \n",
    "    X_test = [item[0] for item in vendors]\n",
    "    items = [item[1] for item in vendors]\n",
    "    \n",
    "    receipt_items, merchant_items = [], []\n",
    "    for i, product in enumerate(items):\n",
    "        product = items[i]\n",
    "        for item in product:\n",
    "            receipt_items.append(item)\n",
    "        for merchant_item in X_test[i]:\n",
    "            merchant_items.append(merchant_item)\n",
    "    \n",
    "    X_test = pd.DataFrame(merchant_items)\n",
    "    vendors_embeddings = convert_to_embeddings_df(X_test)\n",
    "    X_test = vendors_embeddings\n",
    "    \n",
    "    #vendors_embeddings = convert_to_embeddings_df(vendors)\n",
    "    #X_test = vendors_embeddings\n",
    "    \n",
    "    results = pd.DataFrame({'Prediction': cosine_similarity_clf(X_train, y_train, X_test)}).reset_index(drop=True)\n",
    "    vendors = receipts['Vendors'].to_frame()\n",
    "    result_df = pd.concat([vendors, results], axis=1)\n",
    "    return result_df\n",
    "\n",
    "# Dump predictions to csv\n",
    "get_vendor_category().to_csv('./predictions/vendor_category_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12634bd",
   "metadata": {},
   "source": [
    "## Run Product Category Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0af313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_category():\n",
    "    '''\n",
    "    Runs classification of product category on all receipts.\n",
    "    Outputs prediction results to ./predictions/product_category_predictions.csv.\n",
    "    '''\n",
    "    \n",
    "    X_train, y_train = split_database('./databases/product/embedding/embedded_product_database.csv')\n",
    "    \n",
    "    receipts = pd.read_csv('./processed_receipts/vendors_and_products.csv')  \n",
    "    receipts['Products'] = receipts['Products'].apply(eval)\n",
    "    receipts = receipts.apply(process_list, axis=1)\n",
    "    \n",
    "    X_test = [item[0] for item in receipts]\n",
    "    items = [item[1] for item in receipts]\n",
    "    \n",
    "    receipt_items, merchant_items = [], []\n",
    "    for i, product in enumerate(items):\n",
    "        product = items[i]\n",
    "        for item in product:\n",
    "            receipt_items.append(item)\n",
    "        for merchant_item in X_test[i]:\n",
    "            merchant_items.append(merchant_item)\n",
    "    \n",
    "    X_test = pd.DataFrame(merchant_items)\n",
    "    receipt_embeddings = convert_to_embeddings_df(X_test)\n",
    "    X_test = receipt_embeddings\n",
    "\n",
    "    results = pd.DataFrame({'Prediction': cosine_similarity_clf(X_train, y_train, X_test)}).reset_index(drop=True)\n",
    "    receipt_items = pd.DataFrame(receipt_items)\n",
    "    result_df = pd.concat([receipt_items, results], axis=1)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Dump predictions to csv\n",
    "get_product_category().to_csv('./predictions/product_category_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1273d48",
   "metadata": {},
   "source": [
    "# 4) Create tests in python\n",
    "\n",
    "* Functions that just test one test and shows that tests passed/failed\n",
    "* At the end shows how many passed and how many failed\n",
    "\n",
    "- Example:\n",
    "\n",
    "     - handleVendor.py\n",
    "     - all test functions tested in testHandleCategory.py (test all the functions in hangleVendor.py) asserts at the end of each test function\n",
    "\n",
    "     - Fixtures in test file: testing all of the things that are needed for the code to run\n",
    "\n",
    "- For part 2, test for:\n",
    "    - If 7 categories, one of the 7 categories and one of the 7 categories\n",
    "    - Edge cases (ex: error in formatting, must be string in list of possible categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209423a1",
   "metadata": {},
   "source": [
    "# 5) Visualization using Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d4cc416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://168.105.117.251:8501\u001b[0m\n",
      "\u001b[0m\n",
      "^C\n",
      "\u001b[34m  Stopping...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!streamlit run visualization.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
